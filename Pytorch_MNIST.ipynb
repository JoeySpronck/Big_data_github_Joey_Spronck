{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda: pytorch_playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joey/Documents/GitHub/pytorch-playground_aaron-xichen/New Joey\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "#    Part I : Write Data Loaders     #\n",
    "######################################\n",
    "\n",
    "T = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "mnist_data_train = torchvision.datasets.MNIST('mnist_data',\n",
    "                                              transform=T,\n",
    "                                              download=True, \n",
    "                                              train=True)\n",
    "mnist_data_valid = torchvision.datasets.MNIST('mnist_data',\n",
    "                                             transform=T,\n",
    "                                             download=True, \n",
    "                                             train=False)\n",
    "\n",
    "bs = 128\n",
    "mnist_dataloader_train = torch.utils.data.DataLoader(mnist_data_train, batch_size=bs)\n",
    "mnist_dataloader_valid = torch.utils.data.DataLoader(mnist_data_valid, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mnist_data_train))\n",
    "mnist_data_train.targets.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(mnist_data_train))\n",
    "image, label = sample\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap = \"gray\")\n",
    "print(\"label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(mnist_dataloader_train))\n",
    "images, labels = batch\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6,\n",
      "        0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7, 1, 6,\n",
      "        3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9,\n",
      "        3, 1, 1, 0, 4, 9, 2, 0, 0, 2, 0, 2, 7, 1, 8, 6, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = torchvision.utils.make_grid(images, nrow = images.size()[0])\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "print(\"labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#    Part II : Write the Neural Net  #\n",
    "######################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)\n",
    "        \n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features = 60, out_features = 10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.conv1(t)) # HIDDEN CONV LAYER 1\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        t = F.relu(self.conv2(t)) # HIDDEN CONV LAYER 2\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        t = F.relu(self.fc1(self.drop(t.reshape(-1, 12*4*4)))) # HIDDEN FULLY CONNECTED LAYER 1\n",
    "        t = F.relu(self.fc2(self.drop(t))) # HIDDEN FULLY CONNECTED LAYER 2\n",
    "        t = self.out(t) # OUTPUT LAYER\n",
    "        \n",
    "        return t   \n",
    "        \n",
    "#     def forward(self, images):\n",
    "#         # (1) INPUT LAYER\n",
    "#         t = images\n",
    "        \n",
    "#         # (2) HIDDEN CONV LAYER 1\n",
    "#         t = self.conv1(t)\n",
    "#         t = F.relu(t)\n",
    "#         t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "#         # (3) HIDDEN CONV LAYER 2\n",
    "#         t = self.conv2(t)\n",
    "#         t = F.relu(t)\n",
    "#         t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        \n",
    "#         # (4) HIDDEN FULLY CONNECTED LAYER 1\n",
    "#         t = t.reshape(-1, 12*4*4)\n",
    "#         t = self.fc1(t)\n",
    "#         t = F.relu(t)\n",
    "        \n",
    "#         # (5) HIDDEN FULLY CONNECTED LAYER 2\n",
    "#         t = self.fc2(t)\n",
    "#         t = F.relu(t)\n",
    "        \n",
    "#         # (6) OUTPUT LAYER\n",
    "#         t = self.out(t)\n",
    "#         #t = F.softmax(t, dim=1) # NOT NEEDED FOR CE_loss\n",
    "#         return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)\n",
    "print(\"\\n\")\n",
    "print(cnn.conv1)\n",
    "# print(\"\\n\")\n",
    "# print(cnn.conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS ON BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = cnn(images)\n",
    "print(preds.size())\n",
    "#preds\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6,\n",
       "        0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7, 1, 6,\n",
       "        3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9,\n",
       "        3, 1, 1, 0, 4, 9, 2, 0, 0, 2, 0, 2, 7, 1, 8, 6, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "accuracy:  0.1015625 %\n"
     ]
    }
   ],
   "source": [
    "# correct:\n",
    "correct = preds.argmax(dim=1).eq(labels).sum().item()\n",
    "print(correct)\n",
    "# acc: \n",
    "acc = correct/len(labels)\n",
    "print(\"accuracy: \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_accuracy(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3059799671173096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7976e-04, -1.1270e-03, -1.6702e-03, -1.2453e-03, -7.4418e-05],\n",
       "         [-1.2030e-03, -1.8316e-03, -1.7401e-03, -5.6457e-04, -4.0917e-04],\n",
       "         [-1.1455e-03, -1.3458e-03, -4.7658e-04,  2.8992e-04,  2.7511e-04],\n",
       "         [-5.7876e-04, -3.2019e-04, -3.5247e-05,  4.2343e-04, -7.1225e-05],\n",
       "         [-8.3700e-04, -8.0947e-04, -8.7010e-04, -6.5687e-04, -1.8133e-05]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run again after backward\n",
    "print(cnn.conv1.weight.grad.size())\n",
    "cnn.conv1.weight.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.conv1.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params = cnn.parameters(), lr = 0.001)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.conv1.weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# #   Part III : Write Training Loop   #\n",
    "# ######################################\n",
    "\n",
    "# model = CNN()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "# mnist_dataloader_train = torch.utils.data.DataLoader(mnist_data_train, batch_size= 128)\n",
    "\n",
    "# def Get_correct(preds, labels):\n",
    "#     return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "# def Get_accuracy(corrects, samples):\n",
    "#     return Get_correct(preds, labels)/len(labels)\n",
    "\n",
    "# n_epochs=3\n",
    "# n_iterations=0\n",
    "\n",
    "# for e in range(n_epochs):\n",
    "#     total_loss = 0\n",
    "#     total_correct = 0\n",
    "    \n",
    "#     for i, batch in enumerate(mnist_dataloader_train):\n",
    "#         images, labels = batch\n",
    "#         preds = model(images)\n",
    "        \n",
    "#         model.zero_grad()\n",
    "#         loss = loss_function(preds, labels)\n",
    "        \n",
    "#         loss.backward()   \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         total_correct += Get_correct(preds, labels)\n",
    "        \n",
    "#     print(\"[Epoch]:\", e, \"\\n\", \n",
    "#           \"[train loss]:\", loss.item() / bs, \",  \",\n",
    "#           \"[train accuracy]:\", total_correct / len(mnist_data_train)), \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW WITH VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch]: 31 \n",
      " [train loss]: 0.0025235817302018404 ,   [train accuracy]: 0.9324833333333333 \n",
      " [valid loss]: 4.5018903620075434e-05 ,   [valid accuracy]: 0.9751 \n",
      "\n",
      "[Epoch]: 32 \n",
      " [train loss]: 0.0028201404493302107 ,   [train accuracy]: 0.9331 \n",
      " [valid loss]: 4.627387897926383e-05 ,   [valid accuracy]: 0.9748 \n",
      "\n",
      "[Epoch]: 33 \n",
      " [train loss]: 0.004112642724066973 ,   [train accuracy]: 0.9345833333333333 \n",
      " [valid loss]: 4.506087498157285e-05 ,   [valid accuracy]: 0.9748 \n",
      "\n",
      "[Epoch]: 34 \n",
      " [train loss]: 0.0037475135177373886 ,   [train accuracy]: 0.93385 \n",
      " [valid loss]: 4.4834883738076314e-05 ,   [valid accuracy]: 0.9748 \n",
      "\n",
      "[Epoch]: 35 \n",
      " [train loss]: 0.0032885356340557337 ,   [train accuracy]: 0.9343 \n",
      " [valid loss]: 4.493303276831284e-05 ,   [valid accuracy]: 0.9744 \n",
      "\n",
      "[Epoch]: 36 \n",
      " [train loss]: 0.002562715206295252 ,   [train accuracy]: 0.9352166666666667 \n",
      " [valid loss]: 4.6299999667098746e-05 ,   [valid accuracy]: 0.9747 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-a2f33674ce44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_dataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_playground/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#   Part III : Write Training Loop   #\n",
    "######################################\n",
    "\n",
    "model = CNN()\n",
    "load = True\n",
    "if load == True:\n",
    "    load_e = 30\n",
    "    model.load_state_dict(torch.load(f\"model_folder/model_2_{load_e}.pt\"))\n",
    "    ep = load_e + 1 #e_previous\n",
    "else: \n",
    "    ep = 0\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params = model.parameters(), lr = 0.00001)\n",
    "mnist_dataloader_train = torch.utils.data.DataLoader(mnist_data_train, batch_size= 128)\n",
    "\n",
    "def Get_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "def Get_accuracy(corrects, samples):\n",
    "    return Get_correct(preds, labels)/len(labels)\n",
    "\n",
    "n_epochs=15\n",
    "n_iterations=0\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_loss_valid = 0\n",
    "    total_correct_valid = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(mnist_dataloader_train):\n",
    "        images, labels = batch\n",
    "        preds = model(images)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss_train = loss_function(preds, labels)\n",
    "        \n",
    "        loss_train.backward()   \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss_train += loss.item()\n",
    "        total_correct_train += Get_correct(preds, labels)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(mnist_dataloader_valid):\n",
    "            images, labels = batch\n",
    "            preds = model(images)\n",
    "\n",
    "            loss_valid = loss_function(preds, labels)\n",
    "\n",
    "            total_loss_valid += loss.item()\n",
    "            total_correct_valid += Get_correct(preds, labels)    \n",
    "    \n",
    "    model.train()\n",
    "    torch.save(model.state_dict(), f\"model_folder/model_2_{e+ep}.pt\")\n",
    "    \n",
    "    print(\"[Epoch]:\", e+ep, \"\\n\", \n",
    "          \"[train loss]:\", loss_train.item() / bs, \",  \",\n",
    "          \"[train accuracy]:\", total_correct_train / len(mnist_data_train), \"\\n\",\n",
    "          \"[valid loss]:\", loss_valid.item() / bs, \",  \",\n",
    "          \"[valid accuracy]:\", total_correct_valid / len(mnist_data_valid), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = 33\n",
    "model.load_state_dict(torch.load(f\"model_folder/model_2_{save_model}.pt\"))\n",
    "torch.save(model.state_dict(), \"model_folder/model_2_best_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#    Part I : Write Data Loaders     #\n",
    "######################################\n",
    "\n",
    "T = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "emnist_data_train = torchvision.datasets.EMNIST('emnist_data',\n",
    "                                               split = \"balanced\",\n",
    "                                               transform=T,\n",
    "                                               download=True,\n",
    "                                               train=True)\n",
    "emnist_data_valid = torchvision.datasets.EMNIST('emnist_data',\n",
    "                                               split = \"balanced\",\n",
    "                                               transform=T,\n",
    "                                               download=True,\n",
    "                                               train=False)\n",
    "\n",
    "bs = 128\n",
    "emnist_dataloader_train = torch.utils.data.DataLoader(emnist_data_train, batch_size=bs)\n",
    "emnist_dataloader_valid = torch.utils.data.DataLoader(emnist_data_valid, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400,\n",
       "        2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400,\n",
       "        2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400, 2400])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(emnist_data_train))\n",
    "emnist_data_train.targets.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_sample = next(iter(emnist_data_train))\n",
    "e_image, e_label = e_sample\n",
    "e_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(e_image.squeeze(), cmap = \"gray\")\n",
    "print(\"label:\", e_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_batch = next(iter(emnist_dataloader_train))\n",
    "e_images, e_labels = e_batch\n",
    "print(e_images.size())\n",
    "e_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([[-0.0377,  0.0070, -0.0134, -0.0231,  0.0908, -0.0341,  0.0171, -0.0399, -0.0234, -0.0247]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(4)\n",
      "tensor([[0.0970, 0.1015, 0.0994, 0.0984, 0.1103, 0.0974, 0.1025, 0.0968, 0.0984, 0.0983]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "e_pred = cnn(e_image.unsqueeze(dim=0))\n",
    "print(e_pred.size())\n",
    "print(e_pred)\n",
    "print(e_pred.argmax())\n",
    "print(F.softmax(e_pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emnist_data_train.targets.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = torchvision.utils.make_grid(images, nrow = 8)\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "# print(\"labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST batch\n",
    "m_batch = next(iter(mnist_dataloader_train))\n",
    "m_images, m_labels = m_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_drop_list = []\n",
    "for i in range(2):\n",
    "    m_preds = model(m_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMNIST batch\n",
    "e_batch = next(iter(emnist_dataloader_train))\n",
    "e_images, e_labels = e_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5142, -1.0992,  0.8535,  ..., -1.4369,  2.3196, -1.8802],\n",
       "        [ 3.1608, -6.5711,  0.8319,  ..., -4.4226, -3.7382,  1.3921],\n",
       "        [-0.4666, -3.8779,  0.4696,  ..., -4.7831, -1.9859, -3.6750],\n",
       "        ...,\n",
       "        [-8.5964, -2.9795, -0.4570,  ...,  0.4600, -1.9728, -0.9934],\n",
       "        [-3.5361,  1.0887,  7.8504,  ...,  3.3873,  1.2514, -6.0441],\n",
       "        [ 5.2884, -3.5975,  4.6807,  ..., -3.0973, -2.9701, -2.9898]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_preds = model(e_images)\n",
    "e_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NICE, CAT -> MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = torch.empty([1,2,3])\n",
    "second = torch.empty([1,2,3])\n",
    "catted = torch.cat((first, second), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00, -2.0000e+00,  0.0000e+00],\n",
       "         [-2.0000e+00,  1.4013e-44,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00, -2.0000e+00,  0.0000e+00],\n",
       "         [-2.0000e+00,  1.4013e-44,  0.0000e+00]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(catted.shape)\n",
    "catted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -2.0000e+00,  0.0000e+00],\n",
       "        [-2.0000e+00,  1.4013e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catted.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOTHER WAY (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -2.0000e+00,  0.0000e+00],\n",
       "        [-2.0000e+00,  1.4013e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_preds = torch.cat([first, second]).mean(dim=0)\n",
    "m_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_preds = torch.cat([first, second]).std(dim=0)\n",
    "m_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(m_preds.shape)\n",
    "print(m_preds.unsqueeze(dim=0).shape)\n",
    "print(torch.cat((m_preds.unsqueeze(dim=0), m_preds.unsqueeze(dim=0)), dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
